{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "43612103",
   "metadata": {},
   "source": [
    "# Tarea 3\n",
    "\n",
    "**Autor:** Alejandro Zarate Macias  \n",
    "**Curso:** Métodos Matemáticos para Análisis de Datos  \n",
    "**Fecha:** 08 de Septiembre 2025\n",
    "\n",
    "---\n",
    "\n",
    "## Introducción\n",
    "\n",
    "En este notebook se abordan los problemas 3, 6, 7, 8 y 9 de la Tarea 3, relacionados con la optimizacion de multiples funciones y encontrar sus valores minimos o donde la funcion tiende a 0 utilizando el algoritmo de Steepest Descent y diferentes schedulers de learning rate.\n",
    "El objetivo principal es desarrollar códigos que permitan calcular, analizar y visualizar los resultados de estas optimizaciones.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76a65e9b",
   "metadata": {},
   "source": [
    "# Problema 3\n",
    "\n",
    "Escriba un script en Python para generar una matriz simétrica aleatoria $A \\in (-0.5, 0.5)^{10 \\times 10}$. Use la idea del Problema 2 para encontrar un valor adecuado de $\\alpha$ y construir $B$ tal que $B > 0$. Explique por qué la ``estructura interna'' de ambas matrices $A$ y $B$ permanece igual al inspeccionar su espectro.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46cd6a71",
   "metadata": {},
   "source": [
    "## Métodos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffdf3deb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Dimensión de la matriz\n",
    "n = 10\n",
    "\n",
    "# Generar matriz aleatoria en (-0.5, 0.5)\n",
    "A = np.random.uniform(-0.5, 0.5, (n, n))\n",
    "\n",
    "# Hacerla simétrica\n",
    "A = (A + A.T) / 2\n",
    "\n",
    "# Calcular autovalores de A\n",
    "eigvals_A = np.linalg.eigvalsh(A)\n",
    "\n",
    "# Encontrar el menor autovalor\n",
    "lambda_min = np.min(eigvals_A)\n",
    "\n",
    "# Escoger alpha suficientemente grande\n",
    "alpha = abs(lambda_min) + 1e-3\n",
    "\n",
    "# Construir B\n",
    "B = A + alpha * np.eye(n)\n",
    "\n",
    "# Verificar positividad\n",
    "eigvals_B = np.linalg.eigvalsh(B)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "000e47e0",
   "metadata": {},
   "source": [
    "## Resultados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55bbe312",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Matriz A:\\n\", A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef9854c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Autovalores de A:\", eigvals_A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "def66c8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Matriz B (A + alpha*I):\\n\", B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "555e7a7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Autovalores de B:\", eigvals_B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28d6a742",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"¿B es definida positiva?:\", np.all(eigvals_B > 0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2322a40",
   "metadata": {},
   "source": [
    "# Problema 6\n",
    "\n",
    "Implemente en Python el algoritmo de descenso más pronunciado (SD) para las funciones $ (1)-(3) $. Use un paso de longitud fija y el gradiente analítico. Muestre gráficas del número de iteraciones contra el valor de la función.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1f8fe3b",
   "metadata": {},
   "source": [
    "## Métodos\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d17c8319",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e320e28",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SteepestDescent:\n",
    "    def __init__(self): ...\n",
    "    \n",
    "    def optimize(self, \n",
    "                 func, \n",
    "                 grad_func, \n",
    "                 x0, \n",
    "                 lr=0.01,\n",
    "                 max_iterations=1000,\n",
    "                 stop_value=None):\n",
    "        \n",
    "        x = np.array(x0, dtype=float)\n",
    "        trayectory = []\n",
    "        \n",
    "        for _ in range(max_iterations):\n",
    "            f_val = func(x)\n",
    "            trayectory.append(f_val)\n",
    "\n",
    "            if stop_value and f_val <= stop_value:\n",
    "                    break\n",
    "\n",
    "            grad = grad_func(x)\n",
    "                \n",
    "            x = x - lr * grad\n",
    "            \n",
    "        return x, trayectory\n",
    "    \n",
    "    def plot(self, trayectory, title=\"Descenso Más Pronunciado\"):\n",
    "        plt.figure(figsize=(8, 6))\n",
    "        plt.plot(trayectory)\n",
    "        plt.title(title)\n",
    "        plt.xlabel('Iteraciones')\n",
    "        plt.ylabel('Valor de la función')\n",
    "        plt.grid(True)\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea8044e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def translated_sphere(x):\n",
    "    c = np.ones(len(x))\n",
    "    return np.sum((x - c)**2)\n",
    "\n",
    "def gradient_translated_sphere(x):\n",
    "    c = np.ones(len(x))\n",
    "    return 2 * (x - c)\n",
    "\n",
    "def rosenbrock(x):\n",
    "    result = 0\n",
    "    for i in range(len(x) - 1):\n",
    "        result += 100 * (x[i+1] - x[i]**2)**2 + (1 - x[i])**2\n",
    "    return result\n",
    "\n",
    "def gradient_rosenbrock(x):\n",
    "    grad = np.zeros(len(x))\n",
    "    for i in range(len(x) - 1):\n",
    "        grad[i] += -400 * x[i] * (x[i+1] - x[i]**2) - 2 * (1 - x[i])\n",
    "        grad[i+1] += 200 * (x[i+1] - x[i]**2)\n",
    "    return grad\n",
    "\n",
    "def perm(x):\n",
    "    B = 1\n",
    "    result = 0\n",
    "    for k in range(1, len(x) + 1):\n",
    "        inner_sum = 0\n",
    "        for i in range(1, len(x) + 1):\n",
    "            inner_sum += (i + B) * (x[i-1]**k - (1/i)**k)\n",
    "        result += inner_sum**2\n",
    "    return result\n",
    "\n",
    "def gradient_perm(x):\n",
    "    B = 1\n",
    "    grad = np.zeros(len(x))\n",
    "    for j in range(len(x)):\n",
    "        for k in range(1, len(x) + 1):\n",
    "            inner_sum = 0\n",
    "            for i in range(1, len(x) + 1):\n",
    "                inner_sum += (i + B) * (x[i-1]**k - (1/i)**k)\n",
    "            grad[j] += 2 * inner_sum * (j + 1 + B) * k * x[j]**(k-1)\n",
    "    return grad   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3423807c",
   "metadata": {},
   "source": [
    "## Resultados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f873bd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 5\n",
    "x0 = [0.5]*n\n",
    "sd_algorithm = SteepestDescent()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b8c143d",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_sphere, trayectory_sphere = sd_algorithm.optimize(\n",
    "    func=translated_sphere,\n",
    "    grad_func=gradient_translated_sphere,\n",
    "    x0=x0,\n",
    "    lr=0.01,\n",
    "    max_iterations=1000,\n",
    "    stop_value=1e-2\n",
    ")\n",
    "print(\"=\"*60)\n",
    "print(\"FUNCIÓN ESFERA TRASLADADA\")\n",
    "print(\"=\"*60)\n",
    "print(\"Valor óptimo encontrado:\", x_sphere)\n",
    "print(\"Valor de la función en el óptimo:\", translated_sphere(x_sphere))\n",
    "print(\"Número de iteraciones:\", len(trayectory_sphere))\n",
    "print(\"Gráfica Descenso de la función:\")\n",
    "sd_algorithm.plot(trayectory=trayectory_sphere, title=\"Descenso Más Pronunciado - Función Esfera Trasladada\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aea53265",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_rosenbrock, trayectory_rosenbrock = sd_algorithm.optimize(\n",
    "    func=rosenbrock,\n",
    "    grad_func=gradient_rosenbrock,\n",
    "    x0=x0,\n",
    "    lr=0.001,\n",
    "    max_iterations=1000,\n",
    "    stop_value=1e-2\n",
    ")\n",
    "print(\"=\"*60)\n",
    "print(\"FUNCIÓN ROSENBROCK\")\n",
    "print(\"=\"*60)\n",
    "print(\"Valor óptimo encontrado:\", x_rosenbrock)\n",
    "print(\"Valor de la función en el óptimo:\", rosenbrock(x_rosenbrock))\n",
    "print(\"Número de iteraciones:\", len(trayectory_rosenbrock))\n",
    "print(\"Gráfica Descenso de la función:\")\n",
    "sd_algorithm.plot(trayectory=trayectory_rosenbrock, title=\"Descenso Más Pronunciado - Función Rosenbrock\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9c6e8db",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_perm, trayectory_perm = sd_algorithm.optimize(\n",
    "    func=perm,\n",
    "    grad_func=gradient_perm,\n",
    "    x0=x0,\n",
    "    lr=0.001,\n",
    "    max_iterations=1000,\n",
    "    stop_value=1e-2\n",
    ")\n",
    "print(\"=\"*60)\n",
    "print(\"FUNCIÓN PERM\")\n",
    "print(\"=\"*60)\n",
    "print(\"Valor óptimo encontrado:\", x_perm)\n",
    "print(\"Valor de la función en el óptimo:\", perm(x_perm))\n",
    "print(\"Número de iteraciones:\", len(trayectory_perm))\n",
    "print(\"Gráfica Descenso de la función:\")\n",
    "sd_algorithm.plot(trayectory=trayectory_perm, title=\"Descenso Más Pronunciado - Función Perm\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65eef3cb",
   "metadata": {},
   "source": [
    "# Problema 7\n",
    "\n",
    "Mejore su script anterior usando un gradiente numérico en lugar del gradiente analítico. A continuación, resuelva el problema de minimización para las funciones (1)-(3) usando el mismo número de iteraciones que antes. Realice una comparación de las soluciones obtenidas con las del Problema 6."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2affd4cf",
   "metadata": {},
   "source": [
    "## Métodos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e1be048",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43fbe11e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SteepestDescentNumerical:\n",
    "    def __init__(self): ...\n",
    "    \n",
    "    def optimize(self, \n",
    "                 func,  \n",
    "                 x0, \n",
    "                 lr=0.01,\n",
    "                 h=1e-5,\n",
    "                 max_iterations=1000,\n",
    "                 stop_value=None):\n",
    "        \n",
    "        x = np.array(x0, dtype=float)\n",
    "        trayectory = []\n",
    "        \n",
    "        for _ in range(max_iterations):\n",
    "            f_val = func(x)\n",
    "            trayectory.append(f_val)\n",
    "\n",
    "            if stop_value and f_val <= stop_value:\n",
    "                    break\n",
    "\n",
    "            grad = self.numerical_gradient(func, x, h=h)\n",
    "\n",
    "            x = x - lr * grad\n",
    "            \n",
    "        return x, trayectory\n",
    "    \n",
    "    def numerical_gradient(self, f, x, h):\n",
    "        n = len(x)\n",
    "        grad = np.zeros(n)\n",
    "        \n",
    "        for i in range(n):\n",
    "            x_plus = x.copy()\n",
    "            x_minus = x.copy()\n",
    "            \n",
    "            x_plus[i] += h\n",
    "            x_minus[i] -= h\n",
    "\n",
    "            grad[i] = (f(x_plus) - f(x_minus)) / (2*h)\n",
    "\n",
    "        return grad\n",
    "    \n",
    "    def plot(self, trayectory, title=\"Descenso Más Pronunciado\"):\n",
    "        plt.figure(figsize=(8, 6))\n",
    "        plt.plot(trayectory)\n",
    "        plt.title(title)\n",
    "        plt.xlabel('Iteraciones')\n",
    "        plt.ylabel('Valor de la función')\n",
    "        plt.grid(True)\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e79ba615",
   "metadata": {},
   "outputs": [],
   "source": [
    "def translated_sphere(x):\n",
    "    c = np.ones(len(x))\n",
    "    return np.sum((x - c)**2)\n",
    "\n",
    "def rosenbrock(x):\n",
    "    result = 0\n",
    "    for i in range(len(x) - 1):\n",
    "        result += 100 * (x[i+1] - x[i]**2)**2 + (1 - x[i])**2\n",
    "    return result\n",
    "\n",
    "def perm(x):\n",
    "    B = 1\n",
    "    result = 0\n",
    "    for k in range(1, len(x) + 1):\n",
    "        inner_sum = 0\n",
    "        for i in range(1, len(x) + 1):\n",
    "            inner_sum += (i + B) * (x[i-1]**k - (1/i)**k)\n",
    "        result += inner_sum**2\n",
    "    return result  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "018ac980",
   "metadata": {},
   "source": [
    "## Resultados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5202b21e",
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 5\n",
    "x0 = [0.5] * n\n",
    "sd_algorithm = SteepestDescentNumerical()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd2d9f4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_sphere, trayectory_sphere = sd_algorithm.optimize(\n",
    "    func=translated_sphere,\n",
    "    x0=x0,\n",
    "    lr=0.01,\n",
    "    h=1e-5,\n",
    "    max_iterations=1000,\n",
    "    stop_value=1e-2\n",
    ")\n",
    "print(\"=\"*60)\n",
    "print(\"FUNCIÓN ESFERA TRASLADADA\")\n",
    "print(\"=\"*60)\n",
    "print(\"Valor óptimo encontrado:\", x_sphere)\n",
    "print(\"Valor de la función en el óptimo:\", translated_sphere(x_sphere))\n",
    "print(\"Número de iteraciones:\", len(trayectory_sphere))\n",
    "print(\"Gráfica Descenso de la función:\")\n",
    "sd_algorithm.plot(trayectory=trayectory_sphere, title=\"Descenso Más Pronunciado - Función Esfera Trasladada\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5f1f3e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_rosenbrock, trayectory_rosenbrock = sd_algorithm.optimize(\n",
    "    func=rosenbrock,\n",
    "    x0=x0,\n",
    "    lr=0.001,\n",
    "    h=1e-5,\n",
    "    max_iterations=1000,\n",
    "    stop_value=1e-2\n",
    ")\n",
    "print(\"=\"*60)\n",
    "print(\"FUNCIÓN ROSENBROCK\")\n",
    "print(\"=\"*60)\n",
    "print(\"Valor óptimo encontrado:\", x_rosenbrock)\n",
    "print(\"Valor de la función en el óptimo:\", rosenbrock(x_rosenbrock))\n",
    "print(\"Número de iteraciones:\", len(trayectory_rosenbrock))\n",
    "print(\"Gráfica Descenso de la función:\")\n",
    "sd_algorithm.plot(trayectory=trayectory_rosenbrock, title=\"Descenso Más Pronunciado - Función Rosenbrock\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "851de81a",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_perm, trayectory_perm = sd_algorithm.optimize(\n",
    "    func=perm,\n",
    "    x0=x0,\n",
    "    lr=0.001,\n",
    "    h=1e-5,\n",
    "    max_iterations=1000,\n",
    "    stop_value=1e-2\n",
    ")\n",
    "print(\"=\"*60)\n",
    "print(\"FUNCIÓN PERM\")\n",
    "print(\"=\"*60)\n",
    "print(\"Valor óptimo encontrado:\", x_perm)\n",
    "print(\"Valor de la función en el óptimo:\", perm(x_perm))\n",
    "print(\"Número de iteraciones:\", len(trayectory_perm))\n",
    "print(\"Gráfica Descenso de la función:\")\n",
    "sd_algorithm.plot(trayectory=trayectory_perm, title=\"Descenso Más Pronunciado - Función Perm\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "813c9b89",
   "metadata": {},
   "source": [
    "# Problema 8\n",
    "\n",
    "Mejore aún más su script añadiendo longitudes de paso de tipo decreciente lineal, adaptativa e inteligente (i.e.\\ condiciones de Armijo o de Wolfe). Puede usar un gradiente analítico o numérico. Use algunas gráficas para comparar los resultados obtenidos con cada una de las cuatro estrategias de longitud de paso."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c1441f6",
   "metadata": {},
   "source": [
    "## Métodos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "522f026c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1215385b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SteepestDescentNumericalLr:\n",
    "    def __init__(self): ...\n",
    "    \n",
    "    def optimize(self, \n",
    "                 func,  \n",
    "                 x0, \n",
    "                 lr=0.01,\n",
    "                 h=1e-5,\n",
    "                 max_iterations=1000,\n",
    "                 stop_value=None,\n",
    "                 lr_schedule_type='fixed'):\n",
    "        \n",
    "        x = np.array(x0, dtype=float)\n",
    "        trayectory = []\n",
    "        lr_history = []\n",
    "        initial_lr = lr\n",
    "        lr_history.append(initial_lr)\n",
    "        \n",
    "        for i in range(max_iterations):\n",
    "            f_val = func(x)\n",
    "            trayectory.append(f_val)\n",
    "\n",
    "            if stop_value and f_val <= stop_value:\n",
    "                    break\n",
    "\n",
    "            grad = self.numerical_gradient(func, x, h=h)\n",
    "\n",
    "            lr = self.get_learning_rate(\n",
    "                schedule_type=lr_schedule_type,\n",
    "                initial_lr=initial_lr,\n",
    "                iteration=i,\n",
    "                max_iterations=max_iterations,\n",
    "                func=func,\n",
    "                x=x,\n",
    "                grad=grad\n",
    "            )\n",
    "            lr_history.append(lr)\n",
    "\n",
    "            x = x - lr * grad\n",
    "\n",
    "        return x, trayectory, lr_history\n",
    "\n",
    "    def numerical_gradient(self, f, x, h):\n",
    "        n = len(x)\n",
    "        grad = np.zeros(n)\n",
    "        \n",
    "        for i in range(n):\n",
    "            x_plus = x.copy()\n",
    "            x_minus = x.copy()\n",
    "            \n",
    "            x_plus[i] += h\n",
    "            x_minus[i] -= h\n",
    "\n",
    "            grad[i] = (f(x_plus) - f(x_minus)) / (2*h)\n",
    "\n",
    "        return grad\n",
    "    \n",
    "    def get_learning_rate(self, schedule_type, initial_lr, iteration, max_iterations, func, x, grad):\n",
    "        if schedule_type == 'fixed':\n",
    "            return initial_lr\n",
    "        elif schedule_type == 'linear_decreasing':\n",
    "            return self.linear_decreasing_lr(initial_lr, iteration, max_iterations)\n",
    "        elif schedule_type == 'adaptive':\n",
    "            return self.adaptive_lr(initial_lr, iteration)\n",
    "        elif schedule_type == 'intelligent':\n",
    "            return self.intelligent_lr(func, x, grad, initial_lr)\n",
    "        else:\n",
    "            raise ValueError(f\"Estrategia de lr desconocida: {schedule_type}\")\n",
    "\n",
    "    def linear_decreasing_lr(self, initial_lr, iteration, max_iterations):\n",
    "        return initial_lr * (1 - iteration / max_iterations)\n",
    "    \n",
    "    def adaptive_lr(self, initial_lr, iteration):\n",
    "        return initial_lr / (1 + 0.1 * np.sqrt(iteration + 1))\n",
    "\n",
    "    def intelligent_lr(self, func, x, grad, initial_lr):\n",
    "        alpha = initial_lr\n",
    "        c1 = 1e-4\n",
    "        rho = 0.5\n",
    "        max_backtracks = 50\n",
    "        min_alpha = 1e-10\n",
    "        \n",
    "        f_x = func(x)\n",
    "        grad_norm_sq = np.dot(grad, grad)\n",
    "        \n",
    "        if grad_norm_sq < 1e-12:\n",
    "            return min_alpha\n",
    "        \n",
    "        directional_derivative = -grad_norm_sq\n",
    "        backtrack_count = 0\n",
    "        \n",
    "        while backtrack_count < max_backtracks and alpha > min_alpha:\n",
    "            x_new = x - alpha * grad\n",
    "            f_new = func(x_new)\n",
    "\n",
    "            if f_new <= f_x + c1 * alpha * directional_derivative:\n",
    "                break\n",
    "                \n",
    "            alpha *= rho\n",
    "            backtrack_count += 1\n",
    "        \n",
    "        return max(alpha, min_alpha)\n",
    "    \n",
    "    def plot(self, trayectory, title=\"Descenso Más Pronunciado\"):\n",
    "        plt.figure(figsize=(8, 6))\n",
    "        plt.plot(trayectory)\n",
    "        plt.title(title)\n",
    "        plt.xlabel('Iteraciones')\n",
    "        plt.ylabel('Valor de la función')\n",
    "        plt.grid(True)\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ec0d29a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def translated_sphere(x):\n",
    "    c = np.ones(len(x))\n",
    "    return np.sum((x - c)**2)\n",
    "\n",
    "def rosenbrock(x):\n",
    "    result = 0\n",
    "    for i in range(len(x) - 1):\n",
    "        result += 100 * (x[i+1] - x[i]**2)**2 + (1 - x[i])**2\n",
    "    return result\n",
    "\n",
    "def perm(x):\n",
    "    B = 1\n",
    "    result = 0\n",
    "    for k in range(1, len(x) + 1):\n",
    "        inner_sum = 0\n",
    "        for i in range(1, len(x) + 1):\n",
    "            inner_sum += (i + B) * (x[i-1]**k - (1/i)**k)\n",
    "        result += inner_sum**2\n",
    "    return result  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26f39654",
   "metadata": {},
   "source": [
    "## Resultados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c89ca61c",
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 5\n",
    "x0 = [0.5] * n\n",
    "sd_algorithm = SteepestDescentNumericalLr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "893aa54d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"======= OPTIMIZACIÓN DE LA FUNCIÓN ESFERA TRASLADADA =======\")\n",
    "\n",
    "lr_strategies = {\n",
    "    'fixed': 0.1, \n",
    "    'linear_decreasing': 1, \n",
    "    'adaptive': 1, \n",
    "    'intelligent': 5\n",
    "}\n",
    "lr_names = [\"TAMAÑO DE PASO FIJO\", \n",
    "            \"TAMAÑO DE PASO DECRECIENTE LINEALMENTE\",\n",
    "            \"TAMAÑO DE PASO ADAPTATIVO\", \n",
    "            \"TAMAÑO DE PASO INTELIGENTE (BACKTRACKING)\"]\n",
    "\n",
    "trayectories = []\n",
    "lr_histories = []\n",
    "\n",
    "for schedule, lr, name in zip(lr_strategies.keys(), lr_strategies.values(), lr_names):\n",
    "    x_sphere, trayectory_sphere, lr_history_sphere = sd_algorithm.optimize(\n",
    "        func=translated_sphere,\n",
    "        x0=x0,\n",
    "        lr=lr,\n",
    "        h=1e-5,\n",
    "        max_iterations=1000,\n",
    "        stop_value=1e-2,\n",
    "        lr_schedule_type=schedule\n",
    "    )\n",
    "    trayectories.append(trayectory_sphere)\n",
    "    lr_histories.append(lr_history_sphere)\n",
    "    print(\"=\"*60)\n",
    "    print(name)\n",
    "    print(\"=\"*60)\n",
    "    print(\"Valor óptimo encontrado:\", x_sphere)\n",
    "    print(\"Valor de la función en el óptimo:\", translated_sphere(x_sphere))\n",
    "    print(\"Número de iteraciones:\", len(trayectory_sphere))\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"Gráfica Descenso de la función:\")\n",
    "fig, axs = plt.subplots(2, 2, figsize=(15, 10))\n",
    "\n",
    "for ax, history, name in zip(axs.flatten(), trayectories, lr_names):\n",
    "    ax.plot(history, linewidth=2)\n",
    "    ax.set_title(name)\n",
    "    ax.set_xlabel('Iteraciones')\n",
    "    ax.set_ylabel('Valor de la función')\n",
    "    ax.grid(True)\n",
    "\n",
    "plt.suptitle('Función De Esfera Trasladada - Comparación de Estrategias', fontsize=16)\n",
    "plt.show()\n",
    "\n",
    "fig, axs = plt.subplots(2, 2, figsize=(15, 10))\n",
    "for ax, lr_history, name in zip(axs.flatten(), lr_histories, lr_names):\n",
    "    ax.plot(lr_history, linewidth=2)\n",
    "    ax.set_title(f\"Evolución del Tamaño de Paso - {name}\")\n",
    "    ax.set_xlabel('Iteraciones')\n",
    "    ax.set_ylabel('Tamaño de Paso (Learning Rate)')\n",
    "    ax.grid(True)\n",
    "plt.suptitle('Evolución del Tamaño de Paso - Comparación de Estrategias', fontsize=16)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88a27ffe",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"======= OPTIMIZACIÓN DE LA FUNCIÓN DE ROSENBROCK =======\")\n",
    "\n",
    "lr_strategies = {\n",
    "    'fixed': 0.001, \n",
    "    'linear_decreasing': 0.001, \n",
    "    'adaptive': 0.001, \n",
    "    'intelligent': 2\n",
    "}\n",
    "lr_names = [\"TAMAÑO DE PASO FIJO\", \n",
    "            \"TAMAÑO DE PASO DECRECIENTE LINEALMENTE\",\n",
    "            \"TAMAÑO DE PASO ADAPTATIVO\", \n",
    "            \"TAMAÑO DE PASO INTELIGENTE (BACKTRACKING)\"]\n",
    "\n",
    "trayectories = []\n",
    "lr_histories = []\n",
    "\n",
    "for schedule, lr, name in zip(lr_strategies.keys(), lr_strategies.values(), lr_names):\n",
    "    x_rosenbrock, trayectory_rosenbrock, lr_history_rosenbrock = sd_algorithm.optimize(\n",
    "        func=rosenbrock,\n",
    "        x0=x0,\n",
    "        lr=lr,\n",
    "        h=1e-5,\n",
    "        max_iterations=1000,\n",
    "        stop_value=1e-2,\n",
    "        lr_schedule_type=schedule\n",
    "    )\n",
    "    trayectories.append(trayectory_rosenbrock)\n",
    "    lr_histories.append(lr_history_rosenbrock)\n",
    "    print(\"=\"*60)\n",
    "    print(name)\n",
    "    print(\"=\"*60)\n",
    "    print(\"Valor óptimo encontrado:\", x_rosenbrock)\n",
    "    print(\"Valor de la función en el óptimo:\", rosenbrock(x_rosenbrock))\n",
    "    print(\"Número de iteraciones:\", len(trayectory_rosenbrock))\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"Gráfica Descenso de la función:\")\n",
    "fig, axs = plt.subplots(2, 2, figsize=(15, 10))\n",
    "\n",
    "for ax, history, name in zip(axs.flatten(), trayectories, lr_names):\n",
    "    ax.plot(history, linewidth=2)\n",
    "    ax.set_title(name)\n",
    "    ax.set_xlabel('Iteraciones')\n",
    "    ax.set_ylabel('Valor de la función')\n",
    "    ax.grid(True)\n",
    "\n",
    "plt.suptitle('Función De Rosenbrock - Comparación de Estrategias', fontsize=16)\n",
    "plt.show()\n",
    "\n",
    "fig, axs = plt.subplots(2, 2, figsize=(15, 10))\n",
    "for ax, lr_history, name in zip(axs.flatten(), lr_histories, lr_names):\n",
    "    ax.plot(lr_history, linewidth=2)\n",
    "    ax.set_title(f\"Evolución del Tamaño de Paso - {name}\")\n",
    "    ax.set_xlabel('Iteraciones')\n",
    "    ax.set_ylabel('Tamaño de Paso (Learning Rate)')\n",
    "    ax.grid(True)\n",
    "\n",
    "plt.suptitle('Evolución del Tamaño de Paso - Comparación de Estrategias', fontsize=16)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9211baec",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"======= OPTIMIZACIÓN DE LA FUNCIÓN DE PERM =======\")\n",
    "\n",
    "lr_strategies = {\n",
    "    'fixed': 0.001, \n",
    "    'linear_decreasing': 0.001, \n",
    "    'adaptive': 0.001, \n",
    "    'intelligent': 0.5\n",
    "}\n",
    "lr_names = [\"TAMAÑO DE PASO FIJO\", \n",
    "            \"TAMAÑO DE PASO DECRECIENTE LINEALMENTE\",\n",
    "            \"TAMAÑO DE PASO ADAPTATIVO\", \n",
    "            \"TAMAÑO DE PASO INTELIGENTE (BACKTRACKING)\"]\n",
    "\n",
    "trayectories = []\n",
    "lr_histories = []\n",
    "\n",
    "for schedule, lr, name in zip(lr_strategies.keys(), lr_strategies.values(), lr_names):\n",
    "    x_perm, trayectory_perm, lr_history_perm = sd_algorithm.optimize(\n",
    "        func=perm,\n",
    "        x0=x0,\n",
    "        lr=lr,\n",
    "        h=1e-5,\n",
    "        max_iterations=1000,\n",
    "        stop_value=1e-2,\n",
    "        lr_schedule_type=schedule\n",
    "    )\n",
    "    trayectories.append(trayectory_perm)\n",
    "    lr_histories.append(lr_history_perm)\n",
    "    print(\"=\"*60)\n",
    "    print(name)\n",
    "    print(\"=\"*60)\n",
    "    print(\"Valor óptimo encontrado:\", x_perm)\n",
    "    print(\"Valor de la función en el óptimo:\", perm(x_perm))\n",
    "    print(\"Número de iteraciones:\", len(trayectory_perm))\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"Gráfica Descenso de la función:\")\n",
    "fig, axs = plt.subplots(2, 2, figsize=(15, 10))\n",
    "\n",
    "for ax, history, name in zip(axs.flatten(), trayectories, lr_names):\n",
    "    ax.plot(history, linewidth=2)\n",
    "    ax.set_title(name)\n",
    "    ax.set_xlabel('Iteraciones')\n",
    "    ax.set_ylabel('Valor de la función')\n",
    "    ax.grid(True)\n",
    "\n",
    "plt.suptitle('Función De Perm - Comparación de Estrategias', fontsize=16)\n",
    "plt.show()\n",
    "\n",
    "fig, axs = plt.subplots(2, 2, figsize=(15, 10))\n",
    "for ax, lr_history, name in zip(axs.flatten(), lr_histories, lr_names):\n",
    "    ax.plot(lr_history, linewidth=2)\n",
    "    ax.set_title(f\"Evolución del Tamaño de Paso - {name}\")\n",
    "    ax.set_xlabel('Iteraciones')\n",
    "    ax.set_ylabel('Tamaño de Paso (Learning Rate)')\n",
    "    ax.grid(True)\n",
    "\n",
    "plt.suptitle('Evolución del Tamaño de Paso - Comparación de Estrategias', fontsize=16)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f26a4f2",
   "metadata": {},
   "source": [
    "# Problema 9\n",
    "\n",
    "Considere la siguiente función\n",
    "\n",
    "\\begin{align}\n",
    "    f(x_1, x_2) = 10^{9}x_1^{2} + x_2^{2}. \\tag{4}\n",
    "\\end{align}\n",
    "\n",
    "Considere $\\mathbf{x}_0 = (1.5, 1.5)$ como punto inicial. Resuelva el problema de minimización usando su script de $SD$. ¿Cuántas iteraciones necesita su implementación de $SD$ para alcanzar un valor de la función menor que $1e^{-4}$? A continuación, escale las variables de (4). ¿Cuántas iteraciones necesita su implementación de $SD$ para alcanzar un valor menor que $1e^{-4}$ en esta versión escalada de (4)?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c3915c1",
   "metadata": {},
   "source": [
    "## Métodos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69bfb6da",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e216f1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SteepestDescentNumericalLr:\n",
    "    def __init__(self): ...\n",
    "    \n",
    "    def optimize(self, \n",
    "                 func,  \n",
    "                 x0, \n",
    "                 lr=0.01,\n",
    "                 h=1e-5,\n",
    "                 max_iterations=1000,\n",
    "                 stop_value=None,\n",
    "                 lr_schedule_type='fixed'):\n",
    "        \n",
    "        x = np.array(x0, dtype=float)\n",
    "        trayectory = []\n",
    "        lr_history = []\n",
    "        initial_lr = lr\n",
    "        lr_history.append(initial_lr)\n",
    "        \n",
    "        for i in range(max_iterations):\n",
    "            f_val = func(x)\n",
    "            trayectory.append(f_val)\n",
    "\n",
    "            if stop_value and f_val <= stop_value:\n",
    "                    break\n",
    "\n",
    "            grad = self.numerical_gradient(func, x, h=h)\n",
    "\n",
    "            lr = self.get_learning_rate(\n",
    "                schedule_type=lr_schedule_type,\n",
    "                initial_lr=initial_lr,\n",
    "                iteration=i,\n",
    "                max_iterations=max_iterations,\n",
    "                func=func,\n",
    "                x=x,\n",
    "                grad=grad\n",
    "            )\n",
    "            lr_history.append(lr)\n",
    "\n",
    "            x = x - lr * grad\n",
    "\n",
    "        return x, trayectory, lr_history\n",
    "\n",
    "    def numerical_gradient(self, f, x, h):\n",
    "        n = len(x)\n",
    "        grad = np.zeros(n)\n",
    "        \n",
    "        for i in range(n):\n",
    "            x_plus = x.copy()\n",
    "            x_minus = x.copy()\n",
    "            \n",
    "            x_plus[i] += h\n",
    "            x_minus[i] -= h\n",
    "\n",
    "            grad[i] = (f(x_plus) - f(x_minus)) / (2*h)\n",
    "\n",
    "        return grad\n",
    "    \n",
    "    def get_learning_rate(self, schedule_type, initial_lr, iteration, max_iterations, func, x, grad):\n",
    "        if schedule_type == 'fixed':\n",
    "            return initial_lr\n",
    "        elif schedule_type == 'linear_decreasing':\n",
    "            return self.linear_decreasing_lr(initial_lr, iteration, max_iterations)\n",
    "        elif schedule_type == 'adaptive':\n",
    "            return self.adaptive_lr(initial_lr, iteration)\n",
    "        elif schedule_type == 'intelligent':\n",
    "            return self.intelligent_lr(func, x, grad, initial_lr)\n",
    "        else:\n",
    "            raise ValueError(f\"Estrategia de lr desconocida: {schedule_type}\")\n",
    "\n",
    "    def linear_decreasing_lr(self, initial_lr, iteration, max_iterations):\n",
    "        return initial_lr * (1 - iteration / max_iterations)\n",
    "    \n",
    "    def adaptive_lr(self, initial_lr, iteration):\n",
    "        return initial_lr / (1 + 0.1 * np.sqrt(iteration + 1))\n",
    "\n",
    "    def intelligent_lr(self, func, x, grad, initial_lr):\n",
    "        alpha = initial_lr\n",
    "        c1 = 1e-4\n",
    "        rho = 0.5\n",
    "        max_backtracks = 50\n",
    "        min_alpha = 1e-10\n",
    "        \n",
    "        f_x = func(x)\n",
    "        grad_norm_sq = np.dot(grad, grad)\n",
    "        \n",
    "        if grad_norm_sq < 1e-12:\n",
    "            return min_alpha\n",
    "        \n",
    "        directional_derivative = -grad_norm_sq\n",
    "        backtrack_count = 0\n",
    "        \n",
    "        while backtrack_count < max_backtracks and alpha > min_alpha:\n",
    "            x_new = x - alpha * grad\n",
    "            f_new = func(x_new)\n",
    "\n",
    "            if f_new <= f_x + c1 * alpha * directional_derivative:\n",
    "                break\n",
    "                \n",
    "            alpha *= rho\n",
    "            backtrack_count += 1\n",
    "        \n",
    "        return max(alpha, min_alpha)\n",
    "    \n",
    "    def plot(self, trayectory, title=\"Descenso Más Pronunciado\"):\n",
    "        plt.figure(figsize=(8, 6))\n",
    "        plt.plot(trayectory)\n",
    "        plt.title(title)\n",
    "        plt.xlabel('Iteraciones')\n",
    "        plt.ylabel('Valor de la función')\n",
    "        plt.grid(True)\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc31c3e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def function4(x):\n",
    "    \"\"\"10⁹x_1²+x_2²\"\"\"\n",
    "    return 1e9 * x[0]**2 + x[1]**2\n",
    "\n",
    "# MÉTODO 1: ESCALAR LAS VARIABLES f(Cx, Cy)\n",
    "# Para equilibrar los coeficientes, necesitamos que ambos sean similares\n",
    "# Si queremos que ambos coeficientes sean 1, necesitamos:\n",
    "# C₁² * 10⁹ = 1  =>  C₁ = 1/√(10⁹) = 1/31622.77\n",
    "# C₂² * 1 = 1     =>  C₂ = 1\n",
    "\n",
    "import math\n",
    "\n",
    "def function4_normalized(x):\n",
    "    C1 = 1.0 / math.sqrt(1e9)\n",
    "    C2 = 1.0\n",
    "    x1_scaled = C1 * x[0]\n",
    "    x2_scaled = C2 * x[1]\n",
    "    \n",
    "    return 1e9 * x1_scaled**2 + x2_scaled**2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c52f6aa3",
   "metadata": {},
   "source": [
    "## Resultados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04e2dc4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "x0 = [1.5, 1.5]\n",
    "sd_algorithm = SteepestDescentNumericalLr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65b87d4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"======= OPTIMIZACIÓN DE LA FUNCIÓN (4) =======\")\n",
    "\n",
    "lr_strategies = {\n",
    "    'fixed': 1e-10, \n",
    "    'linear_decreasing': 1e-10, \n",
    "    'adaptive': 1e-10, \n",
    "    'intelligent': 1e-1\n",
    "}\n",
    "lr_names = [\"TAMAÑO DE PASO FIJO\", \n",
    "            \"TAMAÑO DE PASO DECRECIENTE LINEALMENTE\",\n",
    "            \"TAMAÑO DE PASO ADAPTATIVO\", \n",
    "            \"TAMAÑO DE PASO INTELIGENTE (BACKTRACKING)\"]\n",
    "\n",
    "trayectories = []\n",
    "lr_histories = []\n",
    "\n",
    "for schedule, lr, name in zip(lr_strategies.keys(), lr_strategies.values(), lr_names):\n",
    "    x_sphere, trayectory_sphere, lr_history_sphere = sd_algorithm.optimize(\n",
    "        func=function4,\n",
    "        x0=x0,\n",
    "        lr=lr,\n",
    "        h=1e-5,\n",
    "        max_iterations=100,\n",
    "        stop_value=1e-4,\n",
    "        lr_schedule_type=schedule\n",
    "    )\n",
    "    trayectories.append(trayectory_sphere)\n",
    "    lr_histories.append(lr_history_sphere)\n",
    "    print(\"=\"*60)\n",
    "    print(name)\n",
    "    print(\"=\"*60)\n",
    "    print(\"Valor óptimo encontrado:\", x_sphere)\n",
    "    print(\"Valor de la función en el óptimo:\", function4(x_sphere))\n",
    "    print(\"Número de iteraciones:\", len(trayectory_sphere))\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"Gráfica Descenso de la función:\")\n",
    "fig, axs = plt.subplots(2, 2, figsize=(15, 10))\n",
    "\n",
    "for ax, history, name in zip(axs.flatten(), trayectories, lr_names):\n",
    "    ax.plot(history, linewidth=2)\n",
    "    ax.set_title(name)\n",
    "    ax.set_xlabel('Iteraciones')\n",
    "    ax.set_ylabel('Valor de la función')\n",
    "    ax.grid(True)\n",
    "\n",
    "plt.suptitle('Función (4) - Comparación de Estrategias', fontsize=16)\n",
    "plt.show()\n",
    "\n",
    "fig, axs = plt.subplots(2, 2, figsize=(15, 10))\n",
    "for ax, lr_history, name in zip(axs.flatten(), lr_histories, lr_names):\n",
    "    ax.plot(lr_history, linewidth=2)\n",
    "    ax.set_title(f\"Evolución del Tamaño de Paso - {name}\")\n",
    "    ax.set_xlabel('Iteraciones')\n",
    "    ax.set_ylabel('Tamaño de Paso (Learning Rate)')\n",
    "    ax.grid(True)\n",
    "plt.suptitle('Evolución del Tamaño de Paso - Comparación de Estrategias', fontsize=16)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd3e3b30",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"======= OPTIMIZACIÓN DE LA FUNCIÓN (4) NORMALIZADA =======\")\n",
    "\n",
    "lr_strategies = {\n",
    "    'fixed': 0.1, \n",
    "    'linear_decreasing': 0.1, \n",
    "    'adaptive': 0.5, \n",
    "    'intelligent': 5\n",
    "}\n",
    "lr_names = [\"TAMAÑO DE PASO FIJO\", \n",
    "            \"TAMAÑO DE PASO DECRECIENTE LINEALMENTE\",\n",
    "            \"TAMAÑO DE PASO ADAPTATIVO\", \n",
    "            \"TAMAÑO DE PASO INTELIGENTE (BACKTRACKING)\"]\n",
    "\n",
    "trayectories = []\n",
    "lr_histories = []\n",
    "\n",
    "for schedule, lr, name in zip(lr_strategies.keys(), lr_strategies.values(), lr_names):\n",
    "    x_sphere, trayectory_sphere, lr_history_sphere = sd_algorithm.optimize(\n",
    "        func=function4_normalized,\n",
    "        x0=x0,\n",
    "        lr=lr,\n",
    "        h=1e-5,\n",
    "        max_iterations=100,\n",
    "        stop_value=1e-4,\n",
    "        lr_schedule_type=schedule\n",
    "    )\n",
    "    trayectories.append(trayectory_sphere)\n",
    "    lr_histories.append(lr_history_sphere)\n",
    "    print(\"=\"*60)\n",
    "    print(name)\n",
    "    print(\"=\"*60)\n",
    "    print(\"Valor óptimo encontrado:\", x_sphere)\n",
    "    print(\"Valor de la función en el óptimo:\", function4_normalized(x_sphere))\n",
    "    print(\"Número de iteraciones:\", len(trayectory_sphere))\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"Gráfica Descenso de la función:\")\n",
    "fig, axs = plt.subplots(2, 2, figsize=(15, 10))\n",
    "\n",
    "for ax, history, name in zip(axs.flatten(), trayectories, lr_names):\n",
    "    ax.plot(history, linewidth=2)\n",
    "    ax.set_title(name)\n",
    "    ax.set_xlabel('Iteraciones')\n",
    "    ax.set_ylabel('Valor de la función')\n",
    "    ax.grid(True)\n",
    "\n",
    "plt.suptitle('Función (4) Normalizada - Comparación de Estrategias', fontsize=16)\n",
    "plt.show()\n",
    "\n",
    "fig, axs = plt.subplots(2, 2, figsize=(15, 10))\n",
    "for ax, lr_history, name in zip(axs.flatten(), lr_histories, lr_names):\n",
    "    ax.plot(lr_history, linewidth=2)\n",
    "    ax.set_title(f\"Evolución del Tamaño de Paso - {name}\")\n",
    "    ax.set_xlabel('Iteraciones')\n",
    "    ax.set_ylabel('Tamaño de Paso (Learning Rate)')\n",
    "    ax.grid(True)\n",
    "plt.suptitle('Evolución del Tamaño de Paso - Comparación de Estrategias', fontsize=16)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "stats",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
