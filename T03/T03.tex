\documentclass{article}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsfonts}
\usepackage{graphicx}
\usepackage{float}

\title{MEMAD-T03}
\author{ALEJANDRO ZARATE MACIAS}
\date{8 de Septiembre 2025}

\begin{document}

\maketitle

% ========================================
% INTRODUCCIÓN
% ========================================
\section*{Introducción}
Para la tarea de esta semana se busca trabajar distintos problemas enfocados en el uso de gradientes y su aplicación en métodos de optimización. El objetivo es poner en práctica lo aprendido en los videos proporcionados como material de estudio, así como en los libros sugeridos para el curso. Todo esto con el fin de aplicar:

\begin{itemize}
    \item Propiedades de convexidad.
    \item Teorema de Taylor.
    \item Gradientes: dirección, tamaño de paso, suficiente descenso, etc.
    \item Condiciones de Armijo y Wolfe
    \item Metodos: Newton, Quasi-Newton y Steepest Descent (descenso mas pronunciado)
\end{itemize}
% ========================================
% SECCIÓN 1
% ========================================
\section{Problema 1}

\subsection{Enunciado}
Supóngase que $f(\mathbf{x}) = \mathbf{x}^T Q \mathbf{x}$, donde $Q \in \mathbb{R}^{n \times n}$, $Q \geq 0$, $Q = Q^T$. Demuestre que $f(\mathbf{x})$ es convexa $\forall \, \mathbf{x} \in \mathbb{R}^n$.

\subsection{Metodología}

Para demostrar la convexidad de $f(\mathbf{x})$, utilizaremos el criterio de la matriz hessiana. Una función es convexa si y solo si su matriz hessiana es semidefinida positiva. Por ende, se deben de realizar los siguientes pasos:
\begin{enumerate}
    \item Calcular el gradiente de $f(\mathbf{x})$.
    \item Calcular la matriz hessiana $\nabla^2 f(\mathbf{x})$.
    \item Demostrar que la hessiana es semidefinida positiva utilizando las propiedades dadas de $Q$.
\end{enumerate}

\subsection{Resultados}
\setcounter{equation}{0}

Comenzamos calculando el gradiente de $f(\mathbf{x}) = \mathbf{x}^T Q \mathbf{x}$.
Para una función cuadrática de la forma $\mathbf{x}^T Q \mathbf{x}$, el gradiente está dado por:
\begin{align}
\nabla f(\mathbf{x}) = (Q + Q^T)\mathbf{x}
\end{align}

Dado que $Q = Q^T$ (la matriz es simétrica), tenemos:
\begin{align}
\nabla f(\mathbf{x}) &= (Q + Q)\mathbf{x} \\
&= 2Q\mathbf{x}
\end{align}

Ahora calculamos la matriz hessiana. La hessiana es la matriz de segundas derivadas parciales:
\begin{align}
\nabla^2 f(\mathbf{x}) &= \frac{\partial}{\partial \mathbf{x}}(\nabla f(\mathbf{x})) \\
&= \frac{\partial}{\partial \mathbf{x}}(2Q\mathbf{x}) \\
&= 2Q
\end{align}

Para que $f(\mathbf{x})$ sea convexa, necesitamos que $\nabla^2 f(\mathbf{x}) \geq 0$ (semidefinida positiva).
Dado que $\nabla^2 f(\mathbf{x}) = 2Q$ y sabemos por hipótesis que $Q \geq 0$ (semidefinida positiva), entonces se confirma que:

\begin{equation*}
    f(\mathbf{x}) = \mathbf{x}^T Q \mathbf{x} \quad \text{Es convexa} \quad \forall \, \mathbf{x} \in \mathbb{R}^n
\end{equation*}

\subsection{Discusión}

El resultado demuestra que la convexidad de $f(\mathbf{x}) = \mathbf{x}^T Q \mathbf{x}$ está directamente relacionada con las propiedades de la matriz $Q$. La clave está en que:

\begin{itemize}
    \item La matriz hessiana resulta ser constante: $\nabla^2 f(\mathbf{x}) = 2Q$
    \item Como $Q \geq 0$ por hipótesis, multiplicar por el escalar positivo 2 preserva la propiedad semidefinida positiva
    \item Al ser la hessiana semidefinida positiva en todo punto, la función es convexa globalmente
\end{itemize}

Este resultado es fundamental en optimización, ya que las funciones cuadráticas con matrices semidefinidas positivas aparecen frecuentemente en problemas de optimización convexa.

\subsection{Conclusión}

Se ha demostrado que $f(\mathbf{x}) = \mathbf{x}^T Q \mathbf{x}$ es convexa para toda $\mathbf{x} \in \mathbb{R}^n$ cuando $Q \geq 0$ y $Q = Q^T$. La demostración se basa en que la matriz hessiana $\nabla^2 f(\mathbf{x}) = 2Q$ es semidefinida positiva, lo cual es una condición suficiente para la convexidad. 


% ========================================
% SECCIÓN 2
% ========================================
\section{Problema 2}

\subsection{Enunciado}
Sea $A \in \mathbb{R}^{n \times n}$, $A = A^T$. Considere  
\begin{align*}
    B = A + \alpha \mathbb{I},
\end{align*}
donde $\mathbb{I} \in \mathbb{R}^{n \times n}$ denota la matriz identidad y $\alpha \in \mathbb{R}^{+}$. Demuestre que $B > 0$ para valores suficientemente grandes de $\alpha$.

\subsection{Metodología}

Para demostrar que $B$ es definida positiva para valores suficientemente grandes de $\alpha$, se pueden utilizar las propiedades de los valores propios:
\begin{enumerate}
    \item Analizar los valores propios de $A$ al ser simétrica.
    \item Determinar cómo afecta la adición de $\alpha \mathbb{I}$ a los valores propios.
    \item Encontrar el valor mínimo de $\alpha$ que garantice que todos los valores propios de $B$ sean positivos.
\end{enumerate}

\subsection{Resultados}
\setcounter{equation}{0}

Sea $A$ una matriz simétrica, por el teorema espectral, se puede escribir como:
\begin{align}
A = Q \Lambda Q^T
\end{align}

donde $Q$ es una matriz ortogonal y $\Lambda = \text{diag}(\lambda_1, \lambda_2, \ldots, \lambda_n)$. Además, podemos reescribir a $\mathbb{I}$ como $QQ^T$ (por las propiedades de la matriz identidad). Cuando formamos $B = A + \alpha \mathbb{I}$, obtenemos:

\begin{align}
B &= A + \alpha \mathbb{I} \\
&= Q \Lambda Q^T + \alpha QQ^T \\
&= Q(\Lambda + \alpha \mathbb{I})Q^T \\
&= Q \text{diag}(\lambda_1 + \alpha, \lambda_2 + \alpha, \ldots, \lambda_n + \alpha) Q^T
\end{align}

Por lo tanto, los valores propios de $B$ son:
\begin{align}
\mu_i = \lambda_i + \alpha, \quad i = 1, 2, \ldots, n
\end{align}

Para que $B$ sea definida positiva ($B > 0$), todos sus valores propios deben ser estrictamente positivos:
\begin{align}
\lambda_i + \alpha > 0 \quad \forall \, i = 1, 2, \ldots, n
\end{align}

Sea $\lambda_{\min} = \min\{\lambda_1, \lambda_2, \ldots, \lambda_n\}$ el menor valor propio de $A$. Entonces:
\begin{align}
\alpha > -\lambda_{\min}
\end{align}

Lo que garantiza que $B > 0$.

\subsection{Discusión}

El resultado muestra que siempre es posible hacer que una matriz simétrica sea definida positiva agregando un múltiplo suficientemente grande de la matriz identidad. Esto es posible porque la adición de $\alpha \mathbb{I}$ desplaza todos los valores propios por la misma cantidad $\alpha$, preservando los vectores propios.

\subsection{Conclusión}

Se ha demostrado que para cualquier matriz simétrica $A$, la matriz $B = A + \alpha \mathbb{I}$ es definida positiva cuando $\alpha > -\lambda_{\min}$, donde $\lambda_{\min}$ es el menor valor propio de $A$. Esta técnica es fundamental en métodos de optimización para regularizar matrices hessianas.

% ========================================
% SECCIÓN 3
% ========================================
\section{Problema 3}

\subsection{Enunciado}
Escriba un script en Python para generar una matriz simétrica aleatoria $A \in (-0.5, 0.5)^{10 \times 10}$. Use la idea del Problema 2 para encontrar un valor adecuado de $\alpha$ y construir $B$ tal que $B > 0$. Explique por qué la ``estructura interna'' de ambas matrices $A$ y $B$ permanece igual al inspeccionar su espectro.

\subsection{Metodología}

Para resolver este problema, se puede realizar un pequeño script con lo siguiente:
\begin{enumerate}
    \item Generar una matriz aleatoria $A \in \mathbb{R}^{10 \times 10}$ con valores entre $(-0.5, 0.5)$ usando numpy.
    \item Hacer que sea simétrica usando la operación $A = \frac{A + A^T}{2}$.
    \item Calcular los valores propios de $A$ utilizando lo que nos ofrece la libreria.
    \item Determinar $\alpha = |\lambda_{\min}| + \epsilon$ donde $\epsilon = 10^{-3}$ para garantizar que $B > 0$.
    \item Construir $B = A + \alpha \mathbb{I}$ y verifica su definida positividad.
\end{enumerate}

\subsection{Resultados}
\setcounter{equation}{0}

Los resultados se pueden encontrar en el archivo "\texttt{t03\_alejandro\_zarate\_macias.ipynb}".

\subsection{Discusión}

La "estructura interna" de las matrices $A$ y $B$ permanece igual al inspeccionar su espectro porque:

\begin{enumerate}
    \item Vectores propios preservados: Al agregar $\alpha \mathbb{I}$ a una matriz simétrica $A$, los vectores propios de $B$ son idénticos a los de $A$. Esto se debe a que:
    \begin{align}
    A\mathbf{v}_i = \lambda_i \mathbf{v}_i \implies (A + \alpha \mathbb{I})\mathbf{v}_i = (\lambda_i + \alpha)\mathbf{v}_i
    \end{align}
    
    \item Desplazamiento espectral uniforme: Los valores propios de $B$ son simplemente los de $A$ desplazados por $\alpha$:
    \begin{align}
    \text{eig}(B) = \text{eig}(A) + \alpha
    \end{align}
    
\end{enumerate}

\subsection{Conclusión}

Se implementó exitosamente un script de pocas líneas que transforma una matriz simétrica en una matriz definida positiva mediante regularización espectral. La técnica preserva la estructura de la matriz original, modificando únicamente la magnitud de los valores propios. Este resultado confirma la teoría del Problema 2.

% ========================================
% SECCIÓN 4
% ========================================
\section{Problema 4}

\subsection{Enunciado}
Explique por qué la idea del Problema 2 falla si $A \neq A^T$.

\subsection{Metodología}

Para explicar por qué la técnica falla con matrices no simétricas, se debe analizar:
\begin{enumerate}
    \item Las diferencias en las propiedades espectrales entre matrices simétricas y no simétricas.
    \item Cómo la condición de definida positiva se relaciona con la simetría.
    \item Proporcionar un ejemplo específico que lo pruebe.
\end{enumerate}

\subsection{Resultados}
\setcounter{equation}{0}

La técnica del Problema 2 falla cuando $A \neq A^T$ por una razón simple: el concepto de matriz "definida positiva" solo se aplica a matrices simétricas.

Cuando $A$ no es simétrica, $B = A + \alpha \mathbb{I}$ tampoco será simétrica. Aunque se pueda hacer que $\mathbf{x}^T B \mathbf{x} > 0$, esto no da las propiedades que se necesitan.

\textbf{Ejemplo:}
\begin{align}
A = \begin{pmatrix}
0 & 1 \\
-1 & 0
\end{pmatrix}, \quad B = \begin{pmatrix}
\alpha & 1 \\
-1 & \alpha
\end{pmatrix}
\end{align}

Para cualquier vector $\mathbf{x} = (x_1, x_2)^T$:
\begin{align}
\mathbf{x}^T B \mathbf{x} = \alpha(x_1^2 + x_2^2)
\end{align}

Aunque esto es positivo para $\alpha > 0$, la matriz $B$ sigue sin ser simétrica. 

El problema es que sin simetría:
\begin{itemize}
    \item No podemos garantizar valores propios reales
    \item La descomposición espectral no funciona igual
    \item Las funciones cuadráticas asociadas pueden no ser convexas
\end{itemize}

\subsection{Discusión}

En resumen, la técnica falla porque necesitamos simetría para tener control sobre los valores propios. Sin simetría, agregar $\alpha \mathbb{I}$ no nos garantiza que la matriz resultante tenga las propiedades que buscamos en optimización.

\subsection{Conclusión}

La idea del Problema 2 no funciona para matrices no simétricas porque el concepto de "definida positiva" requiere simetría. Sin esta propiedad, perdemos las garantías necesarias para optimización convexa.

% ========================================
% SECCIÓN NOTAS
% ========================================
\section*{Importante}
\setcounter{equation}{0}
Para los problemas 5 -- 8 considere las siguientes funciones $f : \mathbb{R}^n \rightarrow \mathbb{R}$:

\begin{itemize}
    \item Función de esfera trasladada:\\
    \begin{align}
        f(\mathbf{x}) = \sum_{i=1}^{n} (x_i - c_i)^2, \quad \text{para un determinado (fijo)} \quad \mathbf{c} \in \mathbb{R}^n.
    \end{align}
    Puede tomarse $\mathbf{c} = (1,1,\dots,1)$, por ejemplo.

    \item Función de Rosenbrock:\\
    \begin{align}
        f(\mathbf{x}) = \sum_{i=1}^{n-1} \Big[ 100(x_{i+1} - x_i^{2})^{2} + (x_i - 1)^{2} \Big].
    \end{align}

    \item Función de Perm $n,\beta$:\\
    \begin{align}
        f(\mathbf{x}) =
        \sum_{i=1}^{n} \left(
            \sum_{j=1}^{n}\,(j^{\,i} + \beta)\left( \left(\frac{x_j}{j}\right)^{i} - 1 \right)
        \right)^{2}, \quad \text{para un determinado (fijo)} \quad \beta \in \mathbb{R}.
    \end{align}
    Puede tomarse $\beta = 1$, por ejemplo.
\end{itemize}
Además, como punto inicial considere $\mathbf{x}_0 = (0.5,0.5,\dots,0.5)$. Asimismo, puede suponerse $n=5$.

% ========================================
% SECCIÓN 5
% ========================================
\section{Problema 5}

\subsection{Enunciado}
Calcule analíticamente $\nabla f(\mathbf{x})$ para las funciones (1)--(3).

\subsection{Metodología}

Para calcular los gradientes analíticamente se procederá función por función:
\begin{enumerate}
    \item Función de esfera trasladada: aplicar la regla de la cadena directamente.
    \item Función de Rosenbrock: usar la regla del producto y la cadena para cada término.
    \item Función de Perm: descomponer las sumas anidadas y aplicar derivación paso a paso.
\end{enumerate}

\subsection{Resultados}
\setcounter{equation}{0}

\textbf{1. Función de esfera trasladada:}
\begin{align}
f(\mathbf{x}) = \sum_{i=1}^{n} (x_i - c_i)^2
\end{align}

Para calcular $\frac{\partial f}{\partial x_j}$:
\begin{align}
\frac{\partial f}{\partial x_j} &= \frac{\partial}{\partial x_j} \sum_{i=1}^{n} (x_i - c_i)^2 \\
&= \sum_{i=1}^{n} \frac{\partial}{\partial x_j} (x_i - c_i)^2 \\
&= 2(x_j - c_j)
\end{align}

Por lo tanto:
\begin{align}
\nabla f(\mathbf{x}) = 2(\mathbf{x} - \mathbf{c})
\end{align}

\textbf{2. Función de Rosenbrock:}
\begin{align}
f(\mathbf{x}) = \sum_{i=1}^{n-1} \Big[ 100(x_{i+1} - x_i^{2})^{2} + (x_i - 1)^{2} \Big]
\end{align}

Para $j = 1$:
\begin{align}
\frac{\partial f}{\partial x_1} &= 100 \cdot 2(x_2 - x_1^2) \cdot (-2x_1) + 2(x_1 - 1) \\
&= -400x_1(x_2 - x_1^2) + 2(x_1 - 1)
\end{align}

Para $1 < j < n-1$:
\begin{align}
\frac{\partial f}{\partial x_j} &= 100 \cdot 2(x_j - x_{j-1}^2) + 100 \cdot 2(x_{j+1} - x_j^2) \cdot (-2x_j) + 2(x_j - 1) \\
&= 200(x_j - x_{j-1}^2) - 400x_j(x_{j+1} - x_j^2) + 2(x_j - 1)
\end{align}

Para $j = n$:
\begin{align}
\frac{\partial f}{\partial x_n} &= 100 \cdot 2(x_n - x_{n-1}^2) \\
&= 200(x_n - x_{n-1}^2)
\end{align}

\textbf{3. Función de Perm $n,\beta$:}
\begin{align}
f(\mathbf{x}) = \sum_{i=1}^{n} \left( \sum_{j=1}^{n}\,(j^{\,i} + \beta)\left( \left(\frac{x_j}{j}\right)^{i} - 1 \right) \right)^{2}
\end{align}

Esta función es más compleja debido a las sumas anidadas. Para simplificar el análisis, definimos:
\begin{align}
S_i = \sum_{j=1}^{n}\,(j^{\,i} + \beta)\left( \left(\frac{x_j}{j}\right)^{i} - 1 \right)
\end{align}

Entonces la función se puede escribir como:
\begin{align}
f(\mathbf{x}) = \sum_{i=1}^{n} S_i^2
\end{align}

Para calcular $\frac{\partial f}{\partial x_k}$, aplicamos la regla de la cadena:
\begin{align}
\frac{\partial f}{\partial x_k} &= \sum_{i=1}^{n} \frac{\partial}{\partial x_k}(S_i^2) = \sum_{i=1}^{n} 2S_i \frac{\partial S_i}{\partial x_k}
\end{align}

Ahora necesitamos calcular $\frac{\partial S_i}{\partial x_k}$. En la suma $S_i$, solo el término con $j = k$ depende de $x_k$:
\begin{align}
\frac{\partial S_i}{\partial x_k} &= \frac{\partial}{\partial x_k}\left[(k^i + \beta)\left( \left(\frac{x_k}{k}\right)^{i} - 1 \right)\right] \\
&= (k^i + \beta) \frac{\partial}{\partial x_k}\left(\frac{x_k}{k}\right)^{i} \\
&= (k^i + \beta) \cdot i \cdot \left(\frac{x_k}{k}\right)^{i-1} \cdot \frac{1}{k} \\
&= (k^i + \beta) \cdot \frac{i}{k} \cdot \left(\frac{x_k}{k}\right)^{i-1}
\end{align}

Sustituyendo de vuelta:
\begin{align}
\frac{\partial f}{\partial x_k} = 2\sum_{i=1}^{n} S_i \cdot (k^i + \beta) \cdot \frac{i}{k} \cdot \left(\frac{x_k}{k}\right)^{i-1}
\end{align}

\textbf{Forma completa del gradiente de la función Perm:}

El $k$-ésimo componente del gradiente es:
\begin{align}
\frac{\partial f}{\partial x_k} = 2\sum_{i=1}^{n} \left[ \sum_{j=1}^{n}\,(j^{\,i} + \beta)\left( \left(\frac{x_j}{j}\right)^{i} - 1 \right) \right] \cdot (k^i + \beta) \cdot \frac{i}{k} \cdot \left(\frac{x_k}{k}\right)^{i-1}
\end{align}

donde $k = 1, 2, \ldots, n$.

\subsection{Discusión}

Los gradientes calculados muestran diferentes niveles de complejidad. La función esfera trasladada tiene el gradiente más simple (lineal), mientras que Rosenbrock y Perm tienen gradientes no lineales más complejos. Esto afectará directamente la velocidad de convergencia de los algoritmos de optimización.

\subsection{Conclusión}

Se han calculado analíticamente los gradientes de las tres funciones. La función esfera trasladada tiene gradiente lineal, Rosenbrock tiene términos cuadráticos y cúbicos, y la función Perm tiene la expresión más compleja con potencias variables dependiendo de los parámetros $i$ y $k$.

% ========================================
% SECCIÓN 6
% ========================================
\section{Problema 6}

\subsection{Enunciado}
Implemente en Python el algoritmo de descenso más pronunciado (SD) para las funciones $ (1)-(3) $. Use un paso de longitud fija y el gradiente analítico. Muestre gráficas del número de iteraciones contra el valor de la función.

\subsection{Metodología}

Se implementará el algoritmo de descenso más pronunciado en Python con las siguientes características:

\begin{enumerate}
    \item \textbf{Clase SteepestDescent}: Algoritmo SD con paso fijo que incluye parámetros configurables (tolerancia, máximo de iteraciones, tamaño de paso) y registro del historial de convergencia.
    
    \item \textbf{Funciones objetivo}: Implementación de las tres funciones según el Problema 5 con $n=5$, $\mathbf{c} = (1,1,1,1,1)$ y $\beta = 1$.
    
    \item \textbf{Gradientes analíticos}: Uso de las derivadas calculadas en el Problema 5 para obtener direcciones de descenso exactas.
    
    \item \textbf{Configuración experimental}: Punto inicial $\mathbf{x}_0 = (0.5, 0.5, 0.5, 0.5, 0.5)$ y paso fijo optimizado empíricamente para cada función.
\end{enumerate}

\subsection{Resultados}
\setcounter{equation}{0}

Las implementaciones del algoritmo de descenso más pronunciado se ejecutaron para las tres funciones objetivo. Los resultados se presentan en las siguientes gráficas que muestran la evolución del valor de la función respecto al número de iteraciones:

\begin{figure}[H]
\centering
\includegraphics[width=0.7\textwidth]{images/6_sphere.png}
\caption{Convergencia del algoritmo SD para la función de esfera trasladada. La función converge rápidamente debido a su naturaleza convexa cuadrática.}
\label{fig:sd_sphere}
\end{figure}

\begin{figure}[H]
\centering
\includegraphics[width=0.7\textwidth]{images/6_rosenbrock.png}
\caption{Convergencia del algoritmo SD para la función de Rosenbrock. Se observa una convergencia más lenta debido a la naturaleza.}
\label{fig:sd_rosenbrock}
\end{figure}

\begin{figure}[H]
\centering
\includegraphics[width=0.7\textwidth]{images/6_perm.png}
\caption{Convergencia del algoritmo SD para la función de Perm. La convergencia presenta comportamiento irregular debido a la complejidad de la función.}
\label{fig:sd_perm}
\end{figure}

Los resultados detallados y la implementación completa del código se encuentran en el archivo \texttt{t03\_alejandro\_zarate\_macias.ipynb}.

\subsection{Discusión}

Los resultados confirman las características de optimización vistas en el Problema 5:

\begin{itemize}
    \item \textbf{Función de esfera}: Como se predijo, resultó la más sencilla de optimizar. Su naturaleza convexa cuadrática permite una convergencia rápida hacia el mínimo global. El gradiente analítico simple $\nabla f(\mathbf{x}) = 2(\mathbf{x} - \mathbf{c})$ facilita direcciones de descenso eficientes.
    
    \item \textbf{Función de Rosenbrock}: Representa un verdadero reto de optimización debido a su curvatura pronunciada. La convergencia es notablemente más lenta, requiriendo un ajuste cuidadoso del tamaño de paso para evitar oscilaciones o que los valores incrementen en lugar de acercarse a 0. El gradiente analítico, aunque complejo, proporciona direcciones precisas pero la geometría de la función limita la eficiencia del método.
    
    \item \textbf{Función de Perm}: Confirma ser un desafío significativo con convergencia irregular. La complejidad de su gradiente analítico con sumas anidadas y potencias variables requiere evaluaciones computacionalmente más costosas.
\end{itemize}

\subsection{Conclusión}

Se implementó exitosamente el algoritmo de descenso más pronunciado con gradientes analíticos para las tres funciones objetivo. Los resultados demuestran la efectividad del método para funciones convexas simples como la esfera, mientras revelan las limitaciones del método para funciones más complejas como Rosenbrock y Perm. El gradiente analítico garantiza precisión en las direcciones de descenso, aunque requiere un esfuerzo previo considerable en la derivación matemática.

% ========================================
% SECCIÓN 7
% ========================================
\section{Problema 7}

\subsection{Enunciado}
Mejore su script anterior usando un gradiente numérico en lugar del gradiente analítico. A continuación, resuelva el problema de minimización para las funciones (1)-(3) usando el mismo número de iteraciones que antes. Realice una comparación de las soluciones obtenidas con las del Problema 6.

\subsection{Metodología}

Se modifica la implementación del Problema 6 reemplazando los gradientes analíticos por gradientes numéricos:

\begin{enumerate}
    \item \textbf{Gradiente numérico}: Implementación usando diferencias finitas centrales:
    \begin{align}
    \frac{\partial f}{\partial x_i} \approx \frac{f(\mathbf{x} + h\mathbf{e}_i) - f(\mathbf{x} - h\mathbf{e}_i)}{2h}
    \end{align}
    donde $h = 10^{-5}$ y $\mathbf{e}_i$ es el vector unitario en la dirección $i$.
    
    \item \textbf{Misma configuración experimental}: Se mantienen los mismos parámetros del Problema 6 (punto inicial, número de iteraciones, tamaño de paso) para permitir comparación directa.
    
    \item \textbf{Análisis comparativo}: Evaluación de convergencia, precisión y costo computacional entre ambos métodos.
\end{enumerate}

\subsection{Resultados}
\setcounter{equation}{0}

Los resultados del algoritmo de descenso más pronunciado con gradiente numérico se presentan en las siguientes gráficas:

\begin{figure}[H]
\centering
\includegraphics[width=0.7\textwidth]{images/7_sphere.png}
\caption{Convergencia del algoritmo SD con gradiente numérico para la función de esfera trasladada. El comportamiento es prácticamente idéntico al gradiente analítico.}
\label{fig:sd_numerical_sphere}
\end{figure}

\begin{figure}[H]
\centering
\includegraphics[width=0.7\textwidth]{images/7_rosenbrock.png}
\caption{Convergencia del algoritmo SD con gradiente numérico para la función de Rosenbrock. Se observa convergencia similar al caso analítico con ligeras variaciones numéricas.}
\label{fig:sd_numerical_rosenbrock}
\end{figure}

\begin{figure}[H]
\centering
\includegraphics[width=0.7\textwidth]{images/7_perm.png}
\caption{Convergencia del algoritmo SD con gradiente numérico para la función de Perm. El comportamiento irregular se mantiene consistente con el gradiente analítico.}
\label{fig:sd_numerical_perm}
\end{figure}

Los resultados detallados y la implementación completa del código se encuentran en el archivo \texttt{t03\_alejandro\_zarate\_macias.ipynb}.

\subsection{Discusión}

La comparación entre gradientes analíticos y numéricos revela resultados notablemente similares:

\begin{itemize}
    \item \textbf{Comportamiento de convergencia}: Las tres funciones exhiben patrones de convergencia prácticamente idénticos entre ambos métodos. Las trayectorias de optimización son casi indistinguibles, confirmando la precisión del gradiente numérico implementado.
    
    \item \textbf{Facilidad de implementación}: El gradiente numérico ofrece una ventaja significativa en términos de implementación: no requiere derivación manual ni implementación específica para cada función. Un solo método genérico puede calcular gradientes para cualquier función diferenciable.
    
    \item \textbf{Robustez}: El método numérico elimina la posibilidad de errores en la derivación manual, especialmente relevante para funciones complejas como la de Perm donde el gradiente analítico involucra múltiples sumas anidadas.
\end{itemize}

\subsection{Conclusión}

Se demostró que el gradiente numérico produce resultados prácticamente idénticos al gradiente analítico para las tres funciones estudiadas, con la ventaja adicional de simplificar significativamente la implementación. La facilidad de no requerir derivación manual hace que el gradiente numérico sea una alternativa muy atractiva para problemas de optimización, especialmente cuando las funciones objetivo son complejas o cuando se necesita un enfoque genérico aplicable a diferentes tipos de funciones.


% ========================================
% SECCIÓN 8
% ========================================
\section{Problema 8}

\subsection{Enunciado}
Mejore aún más su script añadiendo longitudes de paso de tipo decreciente lineal, adaptativa e inteligente (i.e.\ condiciones de Armijo o de Wolfe). Puede usar un gradiente analítico o numérico. Use algunas gráficas para comparar los resultados obtenidos con cada una de las cuatro estrategias de longitud de paso.

\subsection{Metodología}

Se implementan cuatro estrategias diferentes para el tamaño de paso en el algoritmo de descenso más pronunciado:

\begin{enumerate}
    \item \textbf{Paso fijo}: Tamaño de paso constante durante toda la optimización (implementado en problemas anteriores).
    
    \item \textbf{Paso decreciente lineal}: Reducción lineal del tamaño de paso conforme avanzan las iteraciones.
    
    \item \textbf{Paso adaptativo}: Ajuste dinámico del tamaño de paso basado en el progreso de la función objetivo, incrementando cuando hay mejora y reduciendo cuando no la hay.
    
    \item \textbf{Condición de Armijo}: Búsqueda de línea que satisface la condición de descenso suficiente mediante backtracking.
\end{enumerate}

Se compararán estas estrategias usando las mismas tres funciones objetivo (esfera trasladada, Rosenbrock y Perm) con configuración experimental idéntica para evaluar su efectividad relativa.


\subsection{Resultados}
\setcounter{equation}{0}

Los resultados de las cuatro estrategias de tamaño de paso se presentan en las siguientes gráficas de convergencia:

\begin{figure}[H]
\centering
\includegraphics[width=0.7\textwidth]{images/8_sphere.png}
\caption{Comparación de estrategias de tamaño de paso para la función de esfera trasladada. Se observa que todas las estrategias convergen efectivamente, con Armijo mostrando la convergencia más estable.}
\label{fig:step_strategies_sphere}
\end{figure}

\begin{figure}[H]
\centering
\includegraphics[width=0.7\textwidth]{images/8_rosenbrock.png}
\caption{Comparación de estrategias de tamaño de paso para la función de Rosenbrock. Las estrategias adaptativas muestran ventajas significativas sobre el paso fijo en esta función desafiante.}
\label{fig:step_strategies_rosenbrock}
\end{figure}

\begin{figure}[H]
\centering
\includegraphics[width=0.7\textwidth]{images/8_perm.png}
\caption{Comparación de estrategias de tamaño de paso para la función de Perm. La condición de Armijo demuestra mayor robustez en el manejo de la complejidad de esta función.}
\label{fig:step_strategies_perm}
\end{figure}

Adicionalmente, se presenta la evolución del tamaño de paso para cada estrategia:

\begin{figure}[H]
\centering
\includegraphics[width=0.7\textwidth]{images/8_sphere_lr.png}
\caption{Evolución del tamaño de paso (learning rate) para la función de esfera trasladada. Se observa cómo cada estrategia ajusta dinámicamente el paso según su metodología específica.}
\label{fig:lr_evolution_sphere}
\end{figure}

\begin{figure}[H]
\centering
\includegraphics[width=0.7\textwidth]{images/8_rosenbrock_lr.png}
\caption{Evolución del tamaño de paso para la función de Rosenbrock. El método adaptativo y Armijo muestran ajustes más sofisticados que mejoran la convergencia.}
\label{fig:lr_evolution_rosenbrock}
\end{figure}

\begin{figure}[H]
\centering
\includegraphics[width=0.7\textwidth]{images/8_perm_lr.png}
\caption{Evolución del tamaño de paso para la función de Perm. Se evidencia la importancia del control dinámico del paso en funciones complejas.}
\label{fig:lr_evolution_perm}
\end{figure}

Los resultados detallados y la implementación completa del código se encuentran en el archivo \texttt{t03\_alejandro\_zarate\_macias.ipynb}.

\subsection{Discusión}

La comparación de las cuatro estrategias revela patrones importantes sobre el comportamiento del tamaño de paso:

\begin{itemize}
    \item \textbf{Paso fijo}: Proporciona comportamiento predecible pero puede ser subóptimo. Para la función esfera funciona bien, pero para Rosenbrock y Perm puede ser demasiado agresivo o conservador según la región del espacio de búsqueda.
    
    \item \textbf{Paso decreciente lineal}: Ofrece un balance entre exploración inicial (paso grande) y refinamiento final (paso pequeño). Funciona especialmente bien cuando se tiene una estimación del número total de iteraciones necesarias.
    
    \item \textbf{Paso adaptativo}: Demuestra excelente flexibilidad al ajustarse automáticamente según el progreso. Incrementa el paso cuando hay mejora (acelerando convergencia) y lo reduce cuando hay retroceso (evitando divergencia). Particularmente efectivo para Rosenbrock donde la curvatura varía significativamente.
    
    \item \textbf{Condición de Armijo}: Proporciona la estrategia más robusta con garantías teóricas de convergencia. Aunque computacionalmente más costosa (requiere múltiples evaluaciones de función por iteración), ofrece la convergencia más estable y confiable, especialmente para funciones complejas como Perm.
\end{itemize}

\subsection{Conclusión}

Se implementaron exitosamente cuatro estrategias diferentes de tamaño de paso, cada una con características distintivas. Los resultados demuestran que la elección de la estrategia de paso es crucial para el rendimiento del algoritmo, especialmente en funciones no convexas. La condición de Armijo se mostró como la más robusta para funciones complejas, mientras que el paso adaptativo ofreció un excelente balance entre simplicidad de implementación y efectividad. El paso decreciente lineal se mostró como una alternativa práctica cuando se conoce aproximadamente el horizonte de optimización.

% ========================================
% SECCIÓN 9
% ========================================
\section{Problema 9}

\subsection{Enunciado}
Considere la siguiente función

\begin{align}
    f(x_1, x_2) = 10^{9}x_1^{2} + x_2^{2}. \tag{4}
\end{align}

Considere $\mathbf{x}_0 = (1.5, 1.5)$ como punto inicial. Resuelva el problema de minimización usando su script de $SD$. ¿Cuántas iteraciones necesita su implementación de $SD$ para alcanzar un valor de la función menor que $1e^{-4}$? A continuación, escale las variables de (4). ¿Cuántas iteraciones necesita su implementación de $SD$ para alcanzar un valor menor que $1e^{-4}$ en esta versión escalada de (4)?

\subsection{Metodología}

Se analiza el problema de optimización de la función mal condicionada $f(x_1, x_2) = 10^9 x_1^2 + x_2^2$ utilizando dos enfoques:

\begin{enumerate}
    \item \textbf{Función original}: Aplicación directa del algoritmo SD con las cuatro estrategias de tamaño de paso implementadas en el Problema 8.
    
    \item \textbf{Función escalada}: Normalización de variables para equilibrar los coeficientes mediante transformación $x_1' = \frac{x_1}{\sqrt{10^9}}$ y $x_2' = x_2$, resultando en una función mejor condicionada.
    
    \item \textbf{Análisis comparativo}: Evaluación del número de iteraciones requeridas para alcanzar $f(\mathbf{x}) < 10^{-4}$ en ambos casos.
\end{enumerate}

\subsection{Resultados}
\setcounter{equation}{0}

Los resultados de optimización revelan diferencias dramáticas entre la función original y la escalada:

\textbf{Función original ($f(x_1, x_2) = 10^9 x_1^2 + x_2^2$):}

Todas las estrategias de tamaño de paso experimentaron convergencia prematura, quedando estancadas en valores cercanos a $f(\mathbf{x}) \approx 2$. Ninguna estrategia logró alcanzar el criterio de parada $f(\mathbf{x}) < 10^{-4}$, incluso después de aumentar significativamente el número máximo de iteraciones.

\begin{figure}[H]
\centering
\includegraphics[width=0.7\textwidth]{images/9_funcion4.png}
\caption{Convergencia del algoritmo SD para la función original mal condicionada. Todas las estrategias se estancan en valores cercanos a 2 sin alcanzar el criterio de convergencia.}
\label{fig:ill_conditioned_original}
\end{figure}

\textbf{Función escalada:}

La normalización de variables permitió que todas las estrategias alcanzaran exitosamente el criterio de convergencia:

\begin{itemize}
    \item \textbf{Paso fijo}: 26 iteraciones
    \item \textbf{Paso decreciente lineal}: 30 iteraciones  
    \item \textbf{Paso adaptativo}: 4 iteraciones
    \item \textbf{Paso inteligente (Armijo)}: 5 iteraciones
\end{itemize}

\begin{figure}[H]
\centering
\includegraphics[width=0.7\textwidth]{images/9_funcion4_norm.png}
\caption{Convergencia del algoritmo SD para la función escalada. Todas las estrategias convergen exitosamente, con las estrategias adaptativas mostrando convergencia más rápida.}
\label{fig:ill_conditioned_scaled}
\end{figure}

Los resultados detallados y gráficas comparativas se encuentran en el archivo \texttt{t03\_alejandro\_zarate\_macias.ipynb}.

\subsection{Discusión}

Los resultados ilustran claramente el impacto devastador del mal condicionamiento en algoritmos de optimización:

\begin{itemize}
    \item \textbf{Problema de condicionamiento}: La función original presenta un número de condición extremadamente alto ($\kappa \approx 10^9$) debido a la disparidad entre coeficientes. Esto crea un paisaje de optimización con forma de valle muy estrecho y alargado.
    
    \item \textbf{Estancamiento en función original}: El gradiente de la función mal condicionada produce direcciones de descenso que oscilan entre las dos variables sin hacer progreso efectivo hacia el mínimo. El algoritmo queda atrapado en un comportamiento de "zigzag" que impide la convergencia.
    
    \item \textbf{Efectividad del escalado}: La normalización equilibra los gradientes de ambas variables, permitiendo que el algoritmo encuentre direcciones de descenso más eficientes. El número de condición se reduce drásticamente, mejorando la geometría del problema.
\end{itemize}

\subsection{Conclusión}

Este problema demuestra la importancia crítica del preprocesamiento en optimización numérica. El escalado de variables transforma un problema intratable en uno que converge eficientemente con todas las estrategias probadas. Los resultados indican que el mal condicionamiento puede ser más limitante que la elección del algoritmo de optimización, y que técnicas simples de normalización pueden producir mejoras dramáticas en el rendimiento. Las estrategias adaptativas e inteligentes emergieron como las más robustas, especialmente en el contexto de la función escalada.

% ========================================
% SECCIÓN 10
% ========================================
\section{Problema 10}

\subsection{Enunciado}
La diferenciación analítica y numérica no son las únicas formas de calcular derivadas. Investigue un poco sobre la diferenciación automática. Luego, considere la función de Himmelblau:

\begin{align}
    f(x,y) = (x^{2} + y - 11)^{2} + (x + y^{2} - 7)^{2}. \tag{5}
\end{align}

Calcule las derivadas parciales de (5) en el punto $(1,-1)$ usando diferenciación automática (dibuje en papel las gráficas correspondientes y muestre su procedimiento). ¿Qué ventajas y desventajas tienen la diferenciación analítica, numérica y automática al compararlas entre sí?

\subsection{Metodología}

Para aplicar diferenciación automática se seguirá el modo directo (forward mode):
\begin{enumerate}
    \item Descomponer la función en operaciones elementales.
    \item Construir el grafo correspondiente.
    \item Evaluar simultáneamente la función y sus derivadas usando la regla de la cadena.
    \item Comparar los tres métodos de diferenciación.
\end{enumerate}

\subsection{Resultados}
\setcounter{equation}{0}

La función de Himmelblau se puede descomponer como:
\begin{align}
f(x,y) = u_1^2 + u_2^2
\end{align}
donde $u_1 = x^2 + y - 11$ y $u_2 = x + y^2 - 7$.

El grafo computacional correspondiente se muestra en la Figura~\ref{fig:grafo_himmelblau}:

\begin{figure}[H]
\centering
\includegraphics[width=0.5\textwidth]{images/10_autodiff.jpg}
\caption{Grafo computacional para la función de Himmelblau usando diferenciación automática.}
\label{fig:grafo_himmelblau}
\end{figure}

Usando el grafo dibujado, evaluamos en el punto $(1,-1)$:

Para\textbf{ $\frac{\partial f}{\partial x}$:}
Inicializamos con $\dot{x} = 1, \dot{y} = 0$.

\begin{table}[h]
\centering
\begin{tabular}{|l|c|c|l|}
\hline
\textbf{Operación} & \textbf{Valor} & \textbf{Derivada} & \textbf{Cálculo} \\
\hline
$x$ & $1$ & $1$ & Entrada inicial \\
$y$ & $-1$ & $0$ & Entrada inicial \\
\hline
$v_1 = x^2$ & $1$ & $2$ & $\dot{v_1} = 2x\dot{x} = 2(1)(1) = 2$ \\
$v_2 = y^2$ & $1$ & $0$ & $\dot{v_2} = 2y\dot{y} = 2(-1)(0) = 0$ \\
\hline
$v_3 = v_1 + y$ & $0$ & $2$ & $\dot{v_3} = \dot{v_1} + \dot{y} = 2 + 0 = 2$ \\
$v_4 = x + v_2$ & $2$ & $1$ & $\dot{v_4} = \dot{x} + \dot{v_2} = 1 + 0 = 1$ \\
\hline
$v_5 = v_3 - 11$ & $-11$ & $2$ & $\dot{v_5} = \dot{v_3} = 2$ \\
$v_6 = v_4 - 7$ & $-5$ & $1$ & $\dot{v_6} = \dot{v_4} = 1$ \\
\hline
$v_7 = v_5^2$ & $121$ & $-44$ & $\dot{v_7} = 2v_5\dot{v_5} = 2(-11)(2) = -44$ \\
$v_8 = v_6^2$ & $25$ & $-10$ & $\dot{v_8} = 2v_6\dot{v_6} = 2(-5)(1) = -10$ \\
\hline
$f = v_7 + v_8$ & $146$ & $-54$ & $\dot{f} = \dot{v_7} + \dot{v_8} = -44 + (-10) = -54$ \\
\hline
\end{tabular}
\caption{Evaluación forward para $\frac{\partial f}{\partial x}$ en $(1,-1)$}
\end{table}

Para\textbf{ $\frac{\partial f}{\partial y}$:}
Inicializamos con $\dot{x} = 0, \dot{y} = 1$.

\begin{table}[h]
\centering
\begin{tabular}{|l|c|c|l|}
\hline
\textbf{Operación} & \textbf{Valor} & \textbf{Derivada} & \textbf{Cálculo} \\
\hline
$x$ & $1$ & $0$ & Entrada inicial \\
$y$ & $-1$ & $1$ & Entrada inicial \\
\hline
$v_1 = x^2$ & $1$ & $0$ & $\dot{v_1} = 2x\dot{x} = 2(1)(0) = 0$ \\
$v_2 = y^2$ & $1$ & $-2$ & $\dot{v_2} = 2y\dot{y} = 2(-1)(1) = -2$ \\
\hline
$v_3 = v_1 + y$ & $0$ & $1$ & $\dot{v_3} = \dot{v_1} + \dot{y} = 0 + 1 = 1$ \\
$v_4 = x + v_2$ & $2$ & $-2$ & $\dot{v_4} = \dot{x} + \dot{v_2} = 0 + (-2) = -2$ \\
\hline
$v_5 = v_3 - 11$ & $-11$ & $1$ & $\dot{v_5} = \dot{v_3} = 1$ \\
$v_6 = v_4 - 7$ & $-5$ & $-2$ & $\dot{v_6} = \dot{v_4} = -2$ \\
\hline
$v_7 = v_5^2$ & $121$ & $-22$ & $\dot{v_7} = 2v_5\dot{v_5} = 2(-11)(1) = -22$ \\
$v_8 = v_6^2$ & $25$ & $20$ & $\dot{v_8} = 2v_6\dot{v_6} = 2(-5)(-2) = 20$ \\
\hline
$f = v_7 + v_8$ & $146$ & $-2$ & $\dot{f} = \dot{v_7} + \dot{v_8} = -22 + 20 = -2$ \\
\hline
\end{tabular}
\caption{Evaluación forward para $\frac{\partial f}{\partial y}$ en $(1,-1)$}
\end{table}

Proceso paso a paso:

1. Inicialización\textbf{:} Se establecen las variables de entrada $(x,y) = (1,-1)$ y las semillas direccionales $(\dot{x}, \dot{y})$ según la derivada que se quiera calcular.

2. Operaciones\textbf{ }elementales\textbf{:} Cada operación en el grafo se evalúa propagando tanto el valor como su derivada usando las reglas:
   \begin{itemize}
       \item Potenciación: $\frac{d}{dx}(u^n) = nu^{n-1}\frac{du}{dx}$
       \item Suma: $\frac{d}{dx}(u + v) = \frac{du}{dx} + \frac{dv}{dx}$
       \item Constante: $\frac{d}{dx}(u + c) = \frac{du}{dx}$
   \end{itemize}

3. Propagación\textbf{:} Los valores y derivadas se propagan hacia adelante en el grafo hasta llegar a la función objetivo.

Resultados en $(1,-1)$:
\begin{align}
f(1,-1) &= 146 \\
\frac{\partial f}{\partial x}\bigg|_{(1,-1)} &= -54 \\
\frac{\partial f}{\partial y}\bigg|_{(1,-1)} &= -2
\end{align}

\subsection{Discusión}

\textbf{Comparación de métodos de diferenciación:}

\begin{itemize}
    \item Diferenciación Analítica: 
        \begin{itemize}
            \item Ventajas: Exacta, expresión simbólica, eficiente para evaluaciones múltiples
            \item Desventajas: Requiere derivación manual, propensa a errores, compleja para funciones complicadas
        \end{itemize}
    
    \item Diferenciación Numérica:
        \begin{itemize}
            \item Ventajas: Fácil de implementar, aplicable a cualquier función
            \item Desventajas: Errores de truncamiento y redondeo, inestable numéricamente, costosa computacionalmente
        \end{itemize}
    
    \item Diferenciación Automática:
        \begin{itemize}
            \item Ventajas: Exacta hasta precisión de máquina, automática, eficiente
            \item Desventajas: Requiere software especializado, overhead de memoria en modo reverso
        \end{itemize}
\end{itemize}

\subsection{Conclusión}

La diferenciación automática combina las ventajas de los métodos analítico y numérico: proporciona derivadas exactas sin requerir derivación manual. En el ejemplo de la función de Himmelblau, se obtuvieron las derivadas parciales $\frac{\partial f}{\partial x} = -54$ y $\frac{\partial f}{\partial y} = -2$ en $(1,-1)$ de manera sistemática y precisa usando el grafo computacional.

\end{document}
